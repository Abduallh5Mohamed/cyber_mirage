"""
ü§ñ Multi-Agent Reinforcement Learning (MARL)
Multiple AI agents cooperating for better defense
"""

import numpy as np
from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
import torch
import torch.nn as nn
from typing import List, Dict, Tuple
from dataclasses import dataclass


@dataclass
class AgentRole:
    """Define agent roles in the system"""
    name: str
    specialization: str
    priority: float


class MultiAgentSystem:
    """
    Multiple specialized agents working together
    - Detector Agent: Focuses on detecting attacks
    - Collector Agent: Maximizes data collection
    - Decoy Agent: Deploys honeytokens
    - Analyzer Agent: Analyzes patterns
    """
    
    def __init__(self, env):
        self.env = env
        self.agents = {}
        self.roles = [
            AgentRole("detector", "Attack Detection", 0.4),
            AgentRole("collector", "Data Collection", 0.3),
            AgentRole("decoy", "Deception", 0.2),
            AgentRole("analyzer", "Pattern Analysis", 0.1)
        ]
    
    def create_specialized_agents(self):
        """Create specialized agents for different tasks"""
        print("ü§ñ Creating Multi-Agent System...")
        
        for role in self.roles:
            print(f"  Creating {role.name} agent ({role.specialization})...")
            
            # Create agent with specialized reward function
            agent = PPO(
                "MlpPolicy",
                self.env,
                learning_rate=3e-4,
                policy_kwargs=dict(
                    net_arch=dict(pi=[256, 256], vf=[256, 256])
                ),
                verbose=0
            )
            
            self.agents[role.name] = {
                'model': agent,
                'role': role,
                'experience': []
            }
        
        print(f"‚úÖ Created {len(self.agents)} specialized agents")
    
    def train_agents_collaborative(self, timesteps: int = 500_000):
        """Train agents collaboratively"""
        print(f"üéì Training Multi-Agent System ({timesteps} timesteps)...")
        
        for name, agent_data in self.agents.items():
            print(f"\nüîÑ Training {name} agent...")
            agent_data['model'].learn(total_timesteps=timesteps)
            agent_data['model'].save(f"data/models/marl/{name}_agent")
            print(f"‚úÖ {name} agent trained!")
    
    def predict_collaborative(self, obs: np.ndarray) -> int:
        """
        Collaborative decision making
        Each agent votes based on its specialization
        """
        votes = {}
        
        for name, agent_data in self.agents.items():
            action, _ = agent_data['model'].predict(obs, deterministic=True)
            
            if isinstance(action, np.ndarray):
                action = action.item()
            
            # Weight vote by agent priority
            weight = agent_data['role'].priority
            votes[action] = votes.get(action, 0) + weight
        
        # Return action with highest weighted vote
        best_action = max(votes.items(), key=lambda x: x[1])[0]
        return best_action
    
    def evaluate_system(self, n_episodes: int = 10) -> Dict:
        """Evaluate multi-agent system performance"""
        print(f"üìä Evaluating Multi-Agent System ({n_episodes} episodes)...")
        
        episode_rewards = []
        detection_rates = []
        
        for episode in range(n_episodes):
            obs, info = self.env.reset()
            done = False
            total_reward = 0
            
            while not done:
                action = self.predict_collaborative(obs)
                obs, reward, terminated, truncated, info = self.env.step(action)
                total_reward += reward
                done = terminated or truncated
            
            episode_rewards.append(total_reward)
            detection_rates.append(1 if info['detected'] else 0)
        
        results = {
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'detection_rate': np.mean(detection_rates) * 100
        }
        
        print(f"\n‚úÖ Multi-Agent Results:")
        print(f"  Mean Reward: {results['mean_reward']:.2f} ¬± {results['std_reward']:.2f}")
        print(f"  Detection Rate: {results['detection_rate']:.1f}%")
        
        return results


class HierarchicalRL:
    """
    Hierarchical Reinforcement Learning
    High-level strategy + Low-level tactics
    """
    
    def __init__(self, env):
        self.env = env
        self.high_level_agent = None  # Strategic decisions
        self.low_level_agent = None   # Tactical actions
    
    def create_hierarchy(self):
        """Create hierarchical agents"""
        print("üèõÔ∏è Creating Hierarchical RL System...")
        
        # High-level: Strategic planning (every 10 steps)
        print("  Creating high-level strategic agent...")
        self.high_level_agent = PPO(
            "MlpPolicy",
            self.env,
            learning_rate=1e-4,
            policy_kwargs=dict(
                net_arch=dict(pi=[512, 512, 256], vf=[512, 512, 256])
            ),
            verbose=0
        )
        
        # Low-level: Tactical execution (every step)
        print("  Creating low-level tactical agent...")
        self.low_level_agent = PPO(
            "MlpPolicy",
            self.env,
            learning_rate=3e-4,
            policy_kwargs=dict(
                net_arch=dict(pi=[256, 256], vf=[256, 256])
            ),
            verbose=0
        )
        
        print("‚úÖ Hierarchical system created!")
    
    def train_hierarchical(self, timesteps: int = 1_000_000):
        """Train hierarchical agents"""
        print(f"üéì Training Hierarchical System ({timesteps} timesteps)...")
        
        # Train low-level first (learns tactics)
        print("  Training low-level agent...")
        self.low_level_agent.learn(total_timesteps=timesteps // 2)
        
        # Train high-level (learns strategy)
        print("  Training high-level agent...")
        self.high_level_agent.learn(total_timesteps=timesteps // 2)
        
        print("‚úÖ Hierarchical training complete!")


class MetaLearning:
    """
    Meta-Learning (Learning to Learn)
    Quickly adapts to new attacker types
    """
    
    def __init__(self, env):
        self.env = env
        self.meta_model = None
        self.task_models = {}
    
    def create_meta_learner(self):
        """Create meta-learning model"""
        print("üß¨ Creating Meta-Learning System...")
        
        self.meta_model = PPO(
            "MlpPolicy",
            self.env,
            learning_rate=1e-3,
            policy_kwargs=dict(
                net_arch=dict(pi=[512, 512, 512, 256], vf=[512, 512, 512, 256])
            ),
            verbose=0
        )
        
        print("‚úÖ Meta-learner created!")
    
    def fast_adapt(self, new_attacker: str, adaptation_steps: int = 1000):
        """
        Quickly adapt to new attacker type
        MAML-style meta-learning
        """
        print(f"‚ö° Fast adapting to {new_attacker}...")
        
        # Clone meta-model
        adapted_model = PPO.load("data/models/meta_model", env=self.env)
        
        # Fine-tune on new task with few steps
        adapted_model.learn(total_timesteps=adaptation_steps)
        
        # Save adapted model
        self.task_models[new_attacker] = adapted_model
        
        print(f"‚úÖ Adapted to {new_attacker} in {adaptation_steps} steps!")
        
        return adapted_model


class AdversarialTraining:
    """
    Adversarial Training - Self-play
    Agent learns by playing against itself
    """
    
    def __init__(self, env):
        self.env = env
        self.defender = None
        self.attacker = None
    
    def create_adversaries(self):
        """Create defender and attacker agents"""
        print("‚öîÔ∏è Creating Adversarial Training System...")
        
        print("  Creating defender agent...")
        self.defender = PPO(
            "MlpPolicy",
            self.env,
            learning_rate=3e-4,
            verbose=0
        )
        
        print("  Creating attacker agent...")
        self.attacker = PPO(
            "MlpPolicy",
            self.env,
            learning_rate=3e-4,
            verbose=0
        )
        
        print("‚úÖ Adversarial system created!")
    
    def train_adversarial(self, rounds: int = 100):
        """
        Train through adversarial self-play
        Defender tries to detect, Attacker tries to evade
        """
        print(f"‚öîÔ∏è Adversarial Training ({rounds} rounds)...")
        
        for round_num in range(rounds):
            print(f"\nüîÑ Round {round_num + 1}/{rounds}")
            
            # Train defender
            print("  üõ°Ô∏è Training defender...")
            self.defender.learn(total_timesteps=10_000, reset_num_timesteps=False)
            
            # Train attacker (learns from defender's strategy)
            print("  ‚öîÔ∏è Training attacker...")
            self.attacker.learn(total_timesteps=10_000, reset_num_timesteps=False)
            
            # Evaluate
            if (round_num + 1) % 10 == 0:
                print(f"  üìä Evaluation at round {round_num + 1}")
        
        print("\n‚úÖ Adversarial training complete!")


class ContinualLearning:
    """
    Continual Learning - Never stops learning
    Learns from new attacks without forgetting old ones
    """
    
    def __init__(self, env):
        self.env = env
        self.model = None
        self.memory_buffer = []
        self.max_buffer_size = 10000
    
    def create_continual_learner(self):
        """Create continual learning system"""
        print("‚ôæÔ∏è Creating Continual Learning System...")
        
        self.model = PPO(
            "MlpPolicy",
            self.env,
            learning_rate=1e-4,
            verbose=0
        )
        
        print("‚úÖ Continual learner created!")
    
    def learn_from_experience(self, new_data: List):
        """
        Learn from new experiences while preventing catastrophic forgetting
        Uses Experience Replay
        """
        print("üìö Learning from new experiences...")
        
        # Add new experiences to buffer
        self.memory_buffer.extend(new_data)
        
        # Keep buffer size limited
        if len(self.memory_buffer) > self.max_buffer_size:
            self.memory_buffer = self.memory_buffer[-self.max_buffer_size:]
        
        # Train on mixed old + new experiences
        self.model.learn(total_timesteps=1000, reset_num_timesteps=False)
        
        print(f"‚úÖ Learned from {len(new_data)} new experiences")
        print(f"üì¶ Memory buffer: {len(self.memory_buffer)} experiences")


# Example usage
if __name__ == "__main__":
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    
    from environment.comprehensive_env import ComprehensiveHoneynetEnv
    
    print("ü§ñ Advanced Multi-Agent & Meta-Learning Demo")
    print("="*80)
    
    # Create environment
    env = ComprehensiveHoneynetEnv()
    
    # 1. Multi-Agent System
    print("\n1Ô∏è‚É£ Multi-Agent System")
    marl = MultiAgentSystem(env)
    marl.create_specialized_agents()
    # marl.train_agents_collaborative(timesteps=100_000)
    
    # 2. Hierarchical RL
    print("\n2Ô∏è‚É£ Hierarchical RL")
    hrl = HierarchicalRL(env)
    hrl.create_hierarchy()
    # hrl.train_hierarchical(timesteps=100_000)
    
    # 3. Meta-Learning
    print("\n3Ô∏è‚É£ Meta-Learning")
    meta = MetaLearning(env)
    meta.create_meta_learner()
    # meta.fast_adapt("APT_NEW", adaptation_steps=1000)
    
    # 4. Adversarial Training
    print("\n4Ô∏è‚É£ Adversarial Training")
    adv = AdversarialTraining(env)
    adv.create_adversaries()
    # adv.train_adversarial(rounds=10)
    
    # 5. Continual Learning
    print("\n5Ô∏è‚É£ Continual Learning")
    continual = ContinualLearning(env)
    continual.create_continual_learner()
    
    print("\n‚úÖ All advanced RL systems available!")
