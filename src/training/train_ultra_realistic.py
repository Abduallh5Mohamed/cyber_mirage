"""
ğŸ”¥ ULTRA REALISTIC Training Script
ÙŠØ¯Ø±Ù‘Ø¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¹Ù„Ù‰ 16 Ù†ÙˆØ¹ Ù…Ù‡Ø§Ø¬Ù… Ù…Ù† Script Kiddie Ù„Ù€ Equation Group
"""

import os
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from stable_baselines3 import PPO
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.callbacks import CheckpointCallback, EvalCallback
from environment.ultra_realistic_env import UltraRealisticHoneynetEnv
import numpy as np


def make_env():
    """Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ¦Ø©"""
    def _init():
        return UltraRealisticHoneynetEnv()
    return _init


def train_ultra_realistic_model():
    """
    ØªØ¯Ø±ÙŠØ¨ Ù…ÙˆØ¯ÙŠÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ¦Ø© Ø§Ù„ÙˆØ§Ù‚Ø¹ÙŠØ© Ø¬Ø¯Ø§Ù‹
    
    Features:
    - 16 Ù†ÙˆØ¹ Ù…Ù‡Ø§Ø¬Ù… (Script Kiddie â†’ Equation Group)
    - ØªÙˆØ²ÙŠØ¹ ÙˆØ§Ù‚Ø¹ÙŠ (40% beginners, 35% intermediate, 25% advanced/elite)
    - Ù†Ø¸Ø§Ù… Ù…ÙƒØ§ÙØ¢Øª Ù…ØªÙ‚Ø¯Ù…
    - Detection thresholds Ù…ØªØ¯Ø±Ø¬Ø©
    """
    
    print("ğŸ”¥"*40)
    print("ğŸš€ Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ ULTRA REALISTIC MODEL")
    print("ğŸ”¥"*40)
    print()
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø³Ø§Ø±Ø§Øª
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    models_dir = os.path.join(project_root, "data", "models")
    logs_dir = os.path.join(project_root, "data", "logs", "ultra_realistic")
    
    os.makedirs(models_dir, exist_ok=True)
    os.makedirs(logs_dir, exist_ok=True)
    
    print(f"ğŸ“ Models: {models_dir}")
    print(f"ğŸ“Š Logs: {logs_dir}")
    print()
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ¦Ø©
    print("ğŸ—ï¸  Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ¦Ø©...")
    env = DummyVecEnv([make_env()])
    
    # Hyperparameters Ù…Ø­Ø³Ù‘Ù†Ø© Ù„Ù„ÙˆØ§Ù‚Ø¹ÙŠØ© Ø§Ù„Ø¹Ø§Ù„ÙŠØ©
    print("âš™ï¸  Hyperparameters:")
    hyperparams = {
        "learning_rate": 1.5e-4,      # Ø£Ù‚Ù„ Ø´ÙˆÙŠØ© Ù„Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±
        "n_steps": 8192,               # steps Ø£ÙƒØ«Ø± Ù„Ù€ episodes Ø·ÙˆÙŠÙ„Ø©
        "batch_size": 256,             # batch Ø£ÙƒØ¨Ø±
        "n_epochs": 15,                # epochs Ø£ÙƒØ«Ø±
        "gamma": 0.998,                # discount factor Ø¹Ø§Ù„ÙŠ (long-term)
        "gae_lambda": 0.96,
        "clip_range": 0.25,
        "ent_coef": 0.005,             # exploration Ø£Ù‚Ù„
        "vf_coef": 0.6,
        "max_grad_norm": 0.7,
        "policy_kwargs": dict(
            net_arch=[512, 512, 256, 128]  # Ø´Ø¨ÙƒØ© Ø¹Ù…ÙŠÙ‚Ø© Ø¬Ø¯Ø§Ù‹
        ),
        "verbose": 1,
        "tensorboard_log": logs_dir
    }
    
    for key, value in hyperparams.items():
        if key != "policy_kwargs":
            print(f"   {key}: {value}")
        else:
            print(f"   net_arch: {value['net_arch']}")
    print()
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„
    print("ğŸ¤– Ø¥Ù†Ø´Ø§Ø¡ PPO Model...")
    model = PPO(
        "MlpPolicy",
        env,
        **hyperparams
    )
    
    # Callbacks
    checkpoint_callback = CheckpointCallback(
        save_freq=25000,
        save_path=models_dir,
        name_prefix="ultra_realistic_checkpoint"
    )
    
    # Training
    total_timesteps = 750000  # 750K timesteps Ù„Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ ÙƒÙ„ Ø§Ù„Ø£Ù†ÙˆØ§Ø¹
    
    print(f"ğŸ¯ Training for {total_timesteps:,} timesteps...")
    print(f"â±ï¸  Expected time: ~30-45 minutes")
    print()
    print("ğŸ“Š ÙŠØªÙ… ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø¹Ù„Ù‰:")
    print("   ğŸŸ¢ 40% Beginners    (Script kiddies, Defacers, Phishing)")
    print("   ğŸŸ¡ 35% Intermediate (Botnets, Ransomware, Insiders, Financial)")
    print("   ğŸ”´ 15% Advanced     (APT1, APT32, APT34)")
    print("   âš« 10% Elite        (Sandworm, Lazarus, APT28, APT29, Equation)")
    print()
    print("="*80)
    
    try:
        model.learn(
            total_timesteps=total_timesteps,
            callback=checkpoint_callback,
            tb_log_name="ultra_realistic_ppo"
        )
        
        # Ø­ÙØ¸ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ
        final_model_path = os.path.join(models_dir, "ppo_ultra_realistic_final")
        model.save(final_model_path)
        
        print()
        print("="*80)
        print("âœ… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù†ØªÙ‡Ù‰ Ø¨Ù†Ø¬Ø§Ø­!")
        print(f"ğŸ’¾ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ù…Ø­ÙÙˆØ¸ ÙÙŠ: {final_model_path}.zip")
        print()
        
        # Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹
        print("ğŸ§ª Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹ Ø¹Ù„Ù‰ 5 episodes...")
        test_quick_performance(model, env)
        
        print()
        print("ğŸ”¥"*40)
        print("ğŸ‰ ULTRA REALISTIC MODEL Ø¬Ø§Ù‡Ø²!")
        print("ğŸ”¥"*40)
        print()
        print("ğŸ“ Ø§Ù„Ø®Ø·ÙˆØ§Øª Ø§Ù„ØªØ§Ù„ÙŠØ©:")
        print("   1. python src/training/test_realistic.py full    - Ø§Ø®ØªØ¨Ø§Ø± Ø´Ø§Ù…Ù„")
        print("   2. tensorboard --logdir data/logs/ultra_realistic - Ø¹Ø±Ø¶ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª")
        print("   3. Integration Ù…Ø¹ Ø§Ù„ÙØ±ÙŠÙ‚ (7 weeks)")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  ØªÙ… Ø¥ÙŠÙ‚Ø§Ù Ø§Ù„ØªØ¯Ø±ÙŠØ¨!")
        print("ğŸ’¾ Ø­ÙØ¸ Ø§Ù„Ù…ÙˆØ¯ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„ÙŠ...")
        interrupted_path = os.path.join(models_dir, "ppo_ultra_realistic_interrupted")
        model.save(interrupted_path)
        print(f"âœ… Ù…Ø­ÙÙˆØ¸ ÙÙŠ: {interrupted_path}.zip")


def test_quick_performance(model, env):
    """Ø§Ø®ØªØ¨Ø§Ø± Ø³Ø±ÙŠØ¹ Ø¹Ù„Ù‰ 5 episodes"""
    
    results = []
    
    for episode in range(5):
        obs = env.reset()
        done = False
        episode_reward = 0
        steps = 0
        
        while not done and steps < 500:
            action, _ = model.predict(obs, deterministic=True)
            obs, reward, done, info = env.step(action)
            episode_reward += reward[0]
            steps += 1
        
        results.append({
            'episode': episode + 1,
            'reward': episode_reward,
            'steps': steps
        })
        
        print(f"   Episode {episode+1}: Reward={episode_reward:.0f}, Steps={steps}")
    
    avg_reward = np.mean([r['reward'] for r in results])
    avg_steps = np.mean([r['steps'] for r in results])
    
    print(f"\n   ğŸ“Š Average: Reward={avg_reward:.0f}, Steps={avg_steps:.0f}")


if __name__ == "__main__":
    train_ultra_realistic_model()
