"""
ğŸ” Log Collector - Centralized Logging System
Ù†Ø¸Ø§Ù… Ø¬Ù…Ø¹ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø±ÙƒØ²ÙŠ

ÙŠØ¬Ù…Ø¹ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ù…Ù† Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ØµØ§Ø¯Ø± ÙˆÙŠØ­Ù„Ù„Ù‡Ø§
"""

import logging
import json
import gzip
import os
from datetime import datetime
from typing import Dict, List, Optional
from pathlib import Path
import threading
import queue
import hashlib

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LogEntry:
    """Ø¥Ø¯Ø®Ø§Ù„ Ø³Ø¬Ù„"""
    
    def __init__(
        self,
        timestamp: datetime,
        source: str,
        level: str,
        message: str,
        metadata: Dict = None
    ):
        self.timestamp = timestamp
        self.source = source
        self.level = level
        self.message = message
        self.metadata = metadata or {}
        self.entry_id = self._generate_id()
    
    def _generate_id(self) -> str:
        """ØªÙˆÙ„ÙŠØ¯ ID ÙØ±ÙŠØ¯"""
        data = f"{self.timestamp}{self.source}{self.message}"
        return hashlib.sha256(data.encode()).hexdigest()[:16]
    
    def to_dict(self) -> Dict:
        return {
            'id': self.entry_id,
            'timestamp': self.timestamp.isoformat(),
            'source': self.source,
            'level': self.level,
            'message': self.message,
            'metadata': self.metadata
        }
    
    def to_json(self) -> str:
        return json.dumps(self.to_dict())


class LogCollector:
    """
    Ø¬Ø§Ù…Ø¹ Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ù…Ø±ÙƒØ²ÙŠ
    """
    
    def __init__(self, storage_dir: str = "./data/logs"):
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        
        self.log_queue = queue.Queue()
        self.running = False
        self.worker_thread = None
        
        self.current_file = None
        self.current_file_size = 0
        self.max_file_size = 10 * 1024 * 1024  # 10 MB
        
        self.stats = {
            'total_logs': 0,
            'logs_by_source': {},
            'logs_by_level': {}
        }
    
    def start(self):
        """Ø¨Ø¯Ø¡ Ø¬Ø§Ù…Ø¹ Ø§Ù„Ø³Ø¬Ù„Ø§Øª"""
        if self.running:
            logger.warning("Log collector already running")
            return
        
        logger.info("ğŸ” Starting Log Collector")
        self.running = True
        
        self.worker_thread = threading.Thread(target=self._worker_loop)
        self.worker_thread.daemon = True
        self.worker_thread.start()
    
    def stop(self):
        """Ø¥ÙŠÙ‚Ø§Ù Ø¬Ø§Ù…Ø¹ Ø§Ù„Ø³Ø¬Ù„Ø§Øª"""
        logger.info("ğŸ” Stopping Log Collector")
        self.running = False
        
        if self.worker_thread:
            self.worker_thread.join(timeout=5)
        
        self._close_current_file()
    
    def collect(self, log_entry: LogEntry):
        """Ø¬Ù…Ø¹ Ø¥Ø¯Ø®Ø§Ù„ Ø³Ø¬Ù„"""
        self.log_queue.put(log_entry)
    
    def collect_dict(self, log_dict: Dict):
        """Ø¬Ù…Ø¹ Ø³Ø¬Ù„ Ù…Ù† dict"""
        entry = LogEntry(
            timestamp=datetime.fromisoformat(log_dict.get('timestamp', datetime.now().isoformat())),
            source=log_dict.get('source', 'unknown'),
            level=log_dict.get('level', 'INFO'),
            message=log_dict.get('message', ''),
            metadata=log_dict.get('metadata', {})
        )
        self.collect(entry)
    
    def _worker_loop(self):
        """Ø­Ù„Ù‚Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©"""
        while self.running:
            try:
                # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø³Ø¬Ù„ Ù…Ù† Ø§Ù„Ø·Ø§Ø¨ÙˆØ±
                log_entry = self.log_queue.get(timeout=1)
                
                # ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø³Ø¬Ù„
                self._write_log(log_entry)
                
                # ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª
                self._update_stats(log_entry)
                
                self.log_queue.task_done()
                
            except queue.Empty:
                continue
            except Exception as e:
                logger.error(f"Error in worker loop: {e}")
    
    def _write_log(self, log_entry: LogEntry):
        """ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø³Ø¬Ù„ Ø¥Ù„Ù‰ Ù…Ù„Ù"""
        try:
            # ÙØªØ­ Ù…Ù„Ù Ø¬Ø¯ÙŠØ¯ Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
            if self._needs_new_file():
                self._rotate_file()
            
            # ÙƒØªØ§Ø¨Ø© Ø§Ù„Ø³Ø¬Ù„
            if self.current_file:
                line = log_entry.to_json() + '\n'
                self.current_file.write(line)
                self.current_file.flush()
                
                self.current_file_size += len(line)
                
        except Exception as e:
            logger.error(f"Error writing log: {e}")
    
    def _needs_new_file(self) -> bool:
        """Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø­Ø§Ø¬Ø© Ù„Ù…Ù„Ù Ø¬Ø¯ÙŠØ¯"""
        if not self.current_file:
            return True
        
        if self.current_file_size >= self.max_file_size:
            return True
        
        return False
    
    def _rotate_file(self):
        """ØªØ¯ÙˆÙŠØ± Ø§Ù„Ù…Ù„ÙØ§Øª"""
        self._close_current_file()
        
        # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ø³Ù… Ù…Ù„Ù Ø¬Ø¯ÙŠØ¯
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = self.storage_dir / f"logs_{timestamp}.jsonl"
        
        self.current_file = open(filename, 'w')
        self.current_file_size = 0
        
        logger.info(f"Created new log file: {filename}")
    
    def _close_current_file(self):
        """Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø­Ø§Ù„ÙŠ"""
        if self.current_file:
            self.current_file.close()
            
            # Ø¶ØºØ· Ø§Ù„Ù…Ù„Ù
            self._compress_file(self.current_file.name)
            
            self.current_file = None
    
    def _compress_file(self, filename: str):
        """Ø¶ØºØ· Ù…Ù„Ù Ø§Ù„Ø³Ø¬Ù„"""
        try:
            with open(filename, 'rb') as f_in:
                with gzip.open(f"{filename}.gz", 'wb') as f_out:
                    f_out.writelines(f_in)
            
            # Ø­Ø°Ù Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø£ØµÙ„ÙŠ
            os.remove(filename)
            
            logger.info(f"Compressed log file: {filename}.gz")
            
        except Exception as e:
            logger.error(f"Error compressing file: {e}")
    
    def _update_stats(self, log_entry: LogEntry):
        """ØªØ­Ø¯ÙŠØ« Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
        self.stats['total_logs'] += 1
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø­Ø³Ø¨ Ø§Ù„Ù…ØµØ¯Ø±
        source = log_entry.source
        self.stats['logs_by_source'][source] = \
            self.stats['logs_by_source'].get(source, 0) + 1
        
        # Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø­Ø³Ø¨ Ø§Ù„Ù…Ø³ØªÙˆÙ‰
        level = log_entry.level
        self.stats['logs_by_level'][level] = \
            self.stats['logs_by_level'].get(level, 0) + 1
    
    def get_stats(self) -> Dict:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª"""
        return self.stats.copy()
    
    def search_logs(
        self,
        source: Optional[str] = None,
        level: Optional[str] = None,
        start_time: Optional[datetime] = None,
        end_time: Optional[datetime] = None,
        keyword: Optional[str] = None
    ) -> List[Dict]:
        """
        Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
        """
        results = []
        
        try:
            # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø¬Ù…ÙŠØ¹ Ù…Ù„ÙØ§Øª Ø§Ù„Ø³Ø¬Ù„Ø§Øª
            for log_file in self.storage_dir.glob("logs_*.jsonl*"):
                
                # ÙÙƒ Ø§Ù„Ø¶ØºØ· Ø¥Ø°Ø§ Ù„Ø²Ù… Ø§Ù„Ø£Ù…Ø±
                if log_file.suffix == '.gz':
                    with gzip.open(log_file, 'rt') as f:
                        lines = f.readlines()
                else:
                    with open(log_file, 'r') as f:
                        lines = f.readlines()
                
                # Ø§Ù„Ø¨Ø­Ø« ÙÙŠ Ø§Ù„Ø³Ø·ÙˆØ±
                for line in lines:
                    try:
                        entry = json.loads(line)
                        
                        # ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„ÙÙ„Ø§ØªØ±
                        if source and entry.get('source') != source:
                            continue
                        
                        if level and entry.get('level') != level:
                            continue
                        
                        timestamp = datetime.fromisoformat(entry.get('timestamp'))
                        
                        if start_time and timestamp < start_time:
                            continue
                        
                        if end_time and timestamp > end_time:
                            continue
                        
                        if keyword and keyword.lower() not in entry.get('message', '').lower():
                            continue
                        
                        results.append(entry)
                        
                    except json.JSONDecodeError:
                        continue
        
        except Exception as e:
            logger.error(f"Error searching logs: {e}")
        
        return results


class DockerLogCollector:
    """
    Ø¬Ø§Ù…Ø¹ Ø³Ø¬Ù„Ø§Øª Docker Ø§Ù„Ù…ØªØ®ØµØµ
    """
    
    def __init__(self, log_collector: LogCollector):
        self.log_collector = log_collector
    
    def collect_container_logs(self, container_id: str):
        """Ø¬Ù…Ø¹ Ø³Ø¬Ù„Ø§Øª Ø­Ø§ÙˆÙŠØ© Docker"""
        try:
            import docker
            client = docker.from_env()
            container = client.containers.get(container_id)
            
            # Ø¬Ù…Ø¹ Ø§Ù„Ø³Ø¬Ù„Ø§Øª
            logs = container.logs(stream=True)
            
            for log_line in logs:
                entry = LogEntry(
                    timestamp=datetime.now(),
                    source=f"docker:{container_id[:12]}",
                    level="INFO",
                    message=log_line.decode('utf-8').strip(),
                    metadata={'container_id': container_id}
                )
                
                self.log_collector.collect(entry)
                
        except Exception as e:
            logger.error(f"Error collecting Docker logs: {e}")


class NetworkLogCollector:
    """
    Ø¬Ø§Ù…Ø¹ Ø³Ø¬Ù„Ø§Øª Ø§Ù„Ø´Ø¨ÙƒØ©
    """
    
    def __init__(self, log_collector: LogCollector):
        self.log_collector = log_collector
    
    def collect_network_traffic(self, packet_info: Dict):
        """Ø¬Ù…Ø¹ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­Ø²Ù… Ø§Ù„Ø´Ø¨ÙƒØ©"""
        entry = LogEntry(
            timestamp=datetime.now(),
            source="network",
            level="INFO",
            message=f"Network packet: {packet_info.get('protocol')} "
                   f"{packet_info.get('src')} -> {packet_info.get('dst')}",
            metadata=packet_info
        )
        
        self.log_collector.collect(entry)


# Demo
if __name__ == "__main__":
    print("ğŸ” LOG COLLECTOR - DEMO")
    print("="*80)
    
    print("\n1ï¸âƒ£ Creating Log Collector...")
    collector = LogCollector(storage_dir="./data/logs/demo")
    
    print("\n2ï¸âƒ£ Starting collector...")
    collector.start()
    
    print("\n3ï¸âƒ£ Collecting sample logs...")
    
    # Ø³Ø¬Ù„Ø§Øª Ø¹ÙŠÙ†Ø©
    for i in range(10):
        entry = LogEntry(
            timestamp=datetime.now(),
            source="honeypot",
            level="INFO" if i % 3 != 0 else "WARNING",
            message=f"Sample log message #{i+1}",
            metadata={'test': True, 'index': i}
        )
        collector.collect(entry)
    
    print(f"   Collected 10 sample logs")
    
    # Ø§Ù†ØªØ¸Ø§Ø± Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
    import time
    time.sleep(2)
    
    print("\n4ï¸âƒ£ Statistics:")
    stats = collector.get_stats()
    print(f"   Total logs: {stats['total_logs']}")
    print(f"   By source: {stats['logs_by_source']}")
    print(f"   By level: {stats['logs_by_level']}")
    
    print("\n5ï¸âƒ£ Searching logs...")
    results = collector.search_logs(level="WARNING")
    print(f"   Found {len(results)} WARNING logs")
    
    print("\n6ï¸âƒ£ Stopping collector...")
    collector.stop()
    
    print("\nâœ… Demo complete!")
    print(f"   Logs stored in: {collector.storage_dir}")
